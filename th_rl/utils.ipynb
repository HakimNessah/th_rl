{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from th_rl.trainer import create_game\n",
    "import os\n",
    "import pandas\n",
    "import click\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def load_experiment(loc):\n",
    "    cpath = os.path.join(loc, \"config.json\")  # Ajoutez le nom du fichier à la fin du chemin\n",
    "\n",
    "    if not os.path.isfile(cpath) or os.path.basename(loc) == \".DS_Store\":  \n",
    "        # Vérifie si le chemin n'est pas un fichier de configuration valide \n",
    "        # ou s'il s'agit du fichier .DS_Store, dans ce cas, ignore-le.\n",
    "        return None, None, None, None, None  # Gère le cas où cpath n'est pas un fichier de configuration valide\n",
    "\n",
    "    config, agents, environment = create_game(cpath)\n",
    "\n",
    "    for i, agent in enumerate(agents):\n",
    "        agent.load(os.path.join(loc, str(i)))\n",
    "    log = pandas.read_csv(os.path.join(loc, \"log.csv\"))\n",
    "    names = [a[\"name\"] + str(i) for i, a in enumerate(config[\"agents\"])]\n",
    "\n",
    "    rewards = log[[\"rewards\", \"rewards.1\"]].ewm(halflife=1000).mean()\n",
    "    actions = log[[\"actions\", \"actions.1\"]].ewm(halflife=1000).mean()\n",
    "    rewards.columns = names\n",
    "    actions.columns = names\n",
    "    return config, agents, environment, actions, rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def play_game(agents, environment, iters=1):\n",
    "    rewards, actions = [], []\n",
    "    for i in range(iters):\n",
    "        done = False\n",
    "        state = environment.reset()\n",
    "        next_state = state\n",
    "        while not done:\n",
    "            # choose actions\n",
    "            acts = [agent.get_action(next_state) for agent in agents]\n",
    "            # acts = [\n",
    "            #    agent.sample_action(torch.from_numpy(next_state).float())\n",
    "            #    for agent in agents\n",
    "            # ]\n",
    "            scaled_acts = [agent.scale(act) for agent, act in zip(agents, acts)]\n",
    "\n",
    "            # Step through environment\n",
    "            next_state, reward, done = environment.step(scaled_acts)\n",
    "            rewards.append(reward)\n",
    "            actions.append(scaled_acts)\n",
    "\n",
    "    return numpy.array(actions), numpy.array(rewards)\n",
    "\n",
    "\n",
    "def plot_matrix(\n",
    "    x,\n",
    "    y,\n",
    "    z,\n",
    "    title=\"\",\n",
    "    xlabel=\"Actions\",\n",
    "    ylabel=\"States\",\n",
    "    zlabel=\"Values\",\n",
    "    return_fig=False,\n",
    "):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Surface(z=z, x=x, y=y))\n",
    "    fig.update_layout(\n",
    "        scene=dict(xaxis_title=xlabel, yaxis_title=ylabel, zaxis_title=zlabel),\n",
    "        title=title,\n",
    "        width=700,\n",
    "        height=600,\n",
    "        margin=dict(r=20, b=10, l=10, t=30),\n",
    "    )\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_qagent(agent, title=\"\", field=\"value\", return_fig=False):\n",
    "    if field == \"value\":\n",
    "        z = agent.table\n",
    "    else:\n",
    "        z = agent.counter\n",
    "\n",
    "    y = numpy.arange(0, agent.states) / agent.states * agent.max_state\n",
    "    x = agent.action_range[0] + agent.action_space / agent.actions * (\n",
    "        agent.action_range[1] - agent.action_range[0]\n",
    "    )\n",
    "    return plot_matrix(x, y, z, title=title, return_fig=return_fig)\n",
    "\n",
    "\n",
    "#def plot_trajectory(actions, rewards, title=\"\", return_fig=False):\n",
    "def plot_trajectory(actions, rewards, title=\"\", return_fig=False):\n",
    "    print(type(rewards))  # Ajoutez cette ligne pour vérifier le type de données\n",
    "    print(rewards)\n",
    "    rpd = rewards\n",
    "    apd = actions\n",
    "    print(type(rpd))\n",
    "\n",
    "    rpd[\"Total\"] = rpd.sum(axis=1)\n",
    "    rpd[\"Nash\"] = 22.22\n",
    "    rpd[\"Cartel\"] = 25\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=(\"Rewards\", \"Actions\"),\n",
    "    )\n",
    "    for col in rpd.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rpd.index.values, y=rpd[col].values, name=\"Reward {}\".format(col)\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "    for col in apd.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rpd.index.values, y=apd[col].values, name=\"Action {}\".format(col)\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "    fig.update_layout(height=600, width=600, title_text=title)\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve(loc, return_fig=False):\n",
    "    config, agents, environment, actions, rewards = load_experiment(loc)\n",
    "    fig = plot_trajectory(\n",
    "        actions,\n",
    "        rewards,\n",
    "        title=os.path.basename(loc),\n",
    "        return_fig=return_fig,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_learning_curve_conf(loc, return_fig=False):\n",
    "    rewards = []\n",
    "    for f in os.listdir(loc):\n",
    "        log = pandas.read_csv(os.path.join(loc, f, \"log.csv\"))\n",
    "        rewards.append(\n",
    "            log[[\"rewards\", \"rewards.1\"]].ewm(halflife=1000).mean().sum(axis=1)\n",
    "        )\n",
    "    rewards = pandas.concat(rewards, axis=1)\n",
    "    plotdata = pandas.DataFrame()\n",
    "    plotdata[\"median\"] = rewards.quantile(0.5, axis=1)\n",
    "    plotdata[\"75th\"] = rewards.quantile(0.75, axis=1)\n",
    "    plotdata[\"25th\"] = rewards.quantile(0.25, axis=1)\n",
    "    plotdata[\"Nash\"] = 22.22\n",
    "    plotdata[\"Cartel\"] = 25\n",
    "    fig = px.line(plotdata, width=500, height=500, title=os.path.basename(loc))\n",
    "    fig.update_yaxes(range=[10, 25])\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve_sweep(loc, return_fig=False):\n",
    "    plotdata = pandas.DataFrame()\n",
    "    for e in os.listdir(loc):\n",
    "        rewards = []\n",
    "        for f in os.listdir(os.path.join(loc, e)):\n",
    "            log = pandas.read_csv(os.path.join(loc, e, f, \"log.csv\"))\n",
    "            rewards.append(\n",
    "                log[[\"rewards\", \"rewards.1\"]].ewm(halflife=1000).mean().sum(axis=1)\n",
    "            )\n",
    "        rewards = pandas.concat(rewards, axis=1)\n",
    "        plotdata[e + \"-median\"] = rewards.quantile(0.5, axis=1)\n",
    "        # plotdata[e+'-75th'] = rewards.quantile(0.75,axis=1)\n",
    "        # plotdata[e+'-25th'] = rewards.quantile(0.25,axis=1)\n",
    "    plotdata[\"Nash\"] = 22.22\n",
    "    plotdata[\"Cartel\"] = 25\n",
    "    fig = px.line(\n",
    "        plotdata, width=500, height=500, title=\"Learning Curve \" + os.path.basename(loc)\n",
    "    )\n",
    "    fig.update_yaxes(range=[10, 25])\n",
    "    #  position legends inside a plot\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            x=0.3,  # value must be between 0 to 1.\n",
    "            y=0.02,  # value must be between 0 to 1.\n",
    "            traceorder=\"normal\",\n",
    "            font=dict(family=\"sans-serif\", size=10, color=\"black\"),\n",
    "        )\n",
    "    )\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_experiment(loc, return_fig=False):\n",
    "    config, agents, environment, _, _ = load_experiment(loc)\n",
    "    rewards, actions = play_game(agents, environment)\n",
    "    return plot_trajectory(rewards, actions, loc, return_fig)\n",
    "\n",
    "\n",
    "def plot_mean_result(loc, return_fig=False):\n",
    "    expi = os.listdir(loc)\n",
    "    rewards, actions = 0, 0\n",
    "    for exp in expi:\n",
    "        config, agents, environment, _, _ = load_experiment(os.path.join(loc, exp))\n",
    "        acts, rwds = play_game(agents, environment)\n",
    "        rewards += rwds\n",
    "        actions += acts\n",
    "    return plot_trajectory(\n",
    "        actions / len(expi),\n",
    "        rewards / len(expi),\n",
    "        title=os.path.basename(loc),\n",
    "        return_fig=return_fig,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_mean_conf(loc, return_fig=False):\n",
    "    expi = os.listdir(loc)\n",
    "    rewards, actions = [], []\n",
    "    for exp in expi:\n",
    "        config, agents, environment, _, _ = load_experiment(os.path.join(loc, exp))\n",
    "        acts, rwds = play_game(agents, environment)\n",
    "        rewards.append(numpy.sum(rwds, axis=1))\n",
    "        actions.append(acts)\n",
    "    rewards = pandas.DataFrame(data=numpy.stack(rewards, axis=0))\n",
    "    rewards = rewards.ewm(halflife=5, axis=1, min_periods=0).mean()\n",
    "    plotdata = pandas.DataFrame()\n",
    "    plotdata[\"median\"] = rewards.quantile(0.5, axis=0)\n",
    "    plotdata[\"75th\"] = rewards.quantile(0.75, axis=0)\n",
    "    plotdata[\"25th\"] = rewards.quantile(0.25, axis=0)\n",
    "    plotdata[\"Nash\"] = 22.22\n",
    "    plotdata[\"Cartel\"] = 25\n",
    "    fig = px.line(plotdata, width=500, height=500, title=os.path.basename(loc))\n",
    "    fig.update_yaxes(range=[10, 25])\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_visits(loc, return_fig=False):\n",
    "    config, agents, environment, _, _ = load_experiment(loc)\n",
    "    return [plot_qagent(a, loc, \"counter\", return_fig=return_fig) for a in agents]\n",
    "\n",
    "\n",
    "def plot_values(loc, return_fig=False):\n",
    "    config, agents, environment, _, _ = load_experiment(loc)\n",
    "    return [plot_qagent(a, loc, \"value\", return_fig=return_fig) for a in agents]\n",
    "\n",
    "\n",
    "def plot_sweep_conf(loc, return_fig=False):\n",
    "    ptiles = []\n",
    "    for iloc in os.listdir(loc):\n",
    "        exp_loc = os.path.join(loc, iloc)\n",
    "        rewards = []\n",
    "        for exp in os.listdir(exp_loc):\n",
    "            config, agents, environment, _, _ = load_experiment(os.path.join(exp_loc, exp))\n",
    "            acts, rwds = play_game(agents, environment)\n",
    "            rewards.append(numpy.sum(rwds, axis=1))\n",
    "        rewards = numpy.stack(rewards, axis=0)\n",
    "        pt = numpy.percentile(rewards, 50, axis=1)\n",
    "        ptiles.append([numpy.percentile(pt, p) for p in [25, 50, 75]])\n",
    "    plotdata = pandas.DataFrame(data=ptiles, columns=[\"25th\", \"median\", \"75th\"])\n",
    "    plotdata[\"Nash\"] = 22.22\n",
    "    plotdata[\"Cartel\"] = 25\n",
    "    plotdata.index = os.listdir(loc)\n",
    "    fig = px.line(plotdata, width=500, height=500, title=os.path.basename(loc))\n",
    "    fig.update_yaxes(range=[10, 25])\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def calc_discount_nash(discount, freq):\n",
    "    return 22.22222 * (\n",
    "        freq * (1 + (1 - discount) + (1 - discount) ** 2) / 3 + (1 - freq)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--dir\", help=\"Experiment dir\", type=str)\n",
    "@click.option(\"--fun\", default=\"plot_mean_result\", help=\"Experiment dir\", type=str)\n",
    "def main(**params):\n",
    "    eval(params[\"fun\"])(params[\"dir\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_experiment(loc):\n",
    "#    cpath = os.path.join(loc, \"config.json\")\n",
    "#    config, agents, environment = create_game(cpath)\n",
    "#    for i, agent in enumerate(agents):\n",
    "#        agent.load(os.path.join(loc, str(i)))\n",
    "##    log = pandas.read_csv(os.path.join(loc, \"log.csv\"))\n",
    "#    names = [a[\"name\"] + str(i) for i, a in enumerate(config[\"agents\"])]\n",
    "\n",
    "#    rewards = log[[\"rewards\", \"rewards.1\"]].ewm(halflife=1000).mean()\n",
    "#    actions = log[[\"actions\", \"actions.1\"]].ewm(halflife=1000).mean()\n",
    "#    rewards.columns = names\n",
    "#    actions.columns = names\n",
    "#    return config, agents, environment, actions, rewards\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
